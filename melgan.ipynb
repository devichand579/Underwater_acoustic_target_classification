{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20429,"status":"ok","timestamp":1702195943368,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"5tZRuJ12-cAK","outputId":"0fedbfb7-de2c-42ff-eff4-7e77a67307d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14138,"status":"ok","timestamp":1702195957501,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"OHeU0pmLa4sS","outputId":"327a30c2-b815-4985-8cc3-b722ea8fcb48"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -qqq tensorflow_addons\n","!pip install -qqq tensorflow-io"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3241,"status":"ok","timestamp":1702195960738,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"0PVl-ihia4sV","outputId":"cb3af914-487e-4b54-86b4-0d72f20ac4bc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["import tensorflow as tf\n","import tensorflow_io as tfio\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow_addons import layers as addon_layers\n","\n","# Setting logger level to avoid input shape warnings\n","tf.get_logger().setLevel(\"ERROR\")\n","\n","# Defining hyperparameters\n","\n","DESIRED_SAMPLES = 26368\n","LEARNING_RATE_GEN = 1e-5\n","LEARNING_RATE_DISC = 1e-6\n","BATCH_SIZE = 16\n","\n","mse = keras.losses.MeanSquaredError()\n","mae = keras.losses.MeanAbsoluteError()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4536,"status":"ok","timestamp":1702195968240,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"KEIqlM_0a4sW","outputId":"8e7122d9-e177-4f33-ff83-8fbfbc43db9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of audio files: 382\n"]}],"source":["# Splitting the dataset into training and testing splits\n","wavs = tf.io.gfile.glob(\"/content/drive/MyDrive/train_data_real/class_E/*.wav\")\n","print(f\"Number of audio files: {len(wavs)}\")\n","\n","# Mapper function for loading the audio. This function returns two instances of the wave\n","def preprocess(filename):\n","    audio = tf.audio.decode_wav(tf.io.read_file(filename), 1, DESIRED_SAMPLES).audio\n","    return audio, audio\n","\n","\n","# Create tf.data.Dataset objects and apply preprocessing\n","train_dataset = tf.data.Dataset.from_tensor_slices((wavs,))\n","train_dataset = train_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702195971027,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"z_6M2Dz6a4sW"},"outputs":[],"source":["# Custom keras layer for on-the-fly audio to spectrogram conversion\n","\n","\n","class MelSpec(layers.Layer):\n","    def __init__(\n","        self,\n","        frame_length=1024,\n","        frame_step=256,\n","        fft_length=None,\n","        sampling_rate=52734,\n","        num_mel_channels=80,\n","        freq_min=0,\n","        freq_max=8192,\n","        **kwargs,\n","    ):\n","        super().__init__(**kwargs)\n","        self.frame_length = frame_length\n","        self.frame_step = frame_step\n","        self.fft_length = fft_length\n","        self.sampling_rate = sampling_rate\n","        self.num_mel_channels = num_mel_channels\n","        self.freq_min = freq_min\n","        self.freq_max = freq_max\n","        # Defining mel filter. This filter will be multiplied with the STFT output\n","        self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n","            num_mel_bins=self.num_mel_channels,\n","            num_spectrogram_bins=self.frame_length // 2 + 1,\n","            sample_rate=self.sampling_rate,\n","            lower_edge_hertz=self.freq_min,\n","            upper_edge_hertz=self.freq_max,\n","        )\n","\n","    def call(self, audio, training=True):\n","        # We will only perform the transformation during training.\n","        if training:\n","            # Taking the Short Time Fourier Transform. Ensure that the audio is padded.\n","            # In the paper, the STFT output is padded using the 'REFLECT' strategy.\n","            stft = tf.signal.stft(\n","                tf.squeeze(audio, -1),\n","                self.frame_length,\n","                self.frame_step,\n","                self.fft_length,\n","                pad_end=True,\n","            )\n","\n","            # Taking the magnitude of the STFT output\n","            magnitude = tf.abs(stft)\n","\n","            # Multiplying the Mel-filterbank with the magnitude and scaling it using the db scale\n","            mel = tf.matmul(tf.square(magnitude), self.mel_filterbank)\n","            log_mel_spec = tfio.audio.dbscale(mel, top_db=80)\n","            return log_mel_spec\n","        else:\n","            return audio\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"frame_length\": self.frame_length,\n","                \"frame_step\": self.frame_step,\n","                \"fft_length\": self.fft_length,\n","                \"sampling_rate\": self.sampling_rate,\n","                \"num_mel_channels\": self.num_mel_channels,\n","                \"freq_min\": self.freq_min,\n","                \"freq_max\": self.freq_max,\n","            }\n","        )\n","        return config"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702195973012,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"_dhiLO-Xa4sX"},"outputs":[],"source":["# Creating the residual stack block\n","\n","\n","def residual_stack(input, filters):\n","    \"\"\"Convolutional residual stack with weight normalization.\n","\n","    Args:\n","        filters: int, determines filter size for the residual stack.\n","\n","    Returns:\n","        Residual stack output.\n","    \"\"\"\n","    c1 = addon_layers.WeightNormalization(\n","        layers.Conv1D(filters, 3, dilation_rate=1, padding=\"same\"), data_init=False\n","    )(input)\n","    lrelu1 = layers.LeakyReLU()(c1)\n","    c2 = addon_layers.WeightNormalization(\n","        layers.Conv1D(filters, 3, dilation_rate=1, padding=\"same\"), data_init=False\n","    )(lrelu1)\n","    add1 = layers.Add()([c2, input])\n","\n","    lrelu2 = layers.LeakyReLU()(add1)\n","    c3 = addon_layers.WeightNormalization(\n","        layers.Conv1D(filters, 3, dilation_rate=3, padding=\"same\"), data_init=False\n","    )(lrelu2)\n","    lrelu3 = layers.LeakyReLU()(c3)\n","    c4 = addon_layers.WeightNormalization(\n","        layers.Conv1D(filters, 3, dilation_rate=1, padding=\"same\"), data_init=False\n","    )(lrelu3)\n","    add2 = layers.Add()([add1, c4])\n","\n","    lrelu4 = layers.LeakyReLU()(add2)\n","    c5 = addon_layers.WeightNormalization(\n","        layers.Conv1D(filters, 3, dilation_rate=9, padding=\"same\"), data_init=False\n","    )(lrelu4)\n","    lrelu5 = layers.LeakyReLU()(c5)\n","    c6 = addon_layers.WeightNormalization(\n","        layers.Conv1D(filters, 3, dilation_rate=1, padding=\"same\"), data_init=False\n","    )(lrelu5)\n","    add3 = layers.Add()([c6, add2])\n","\n","    return add3"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702195974388,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"qtGcsJlFa4sY"},"outputs":[],"source":["# Dilated convolutional block consisting of the Residual stack\n","\n","\n","def conv_block(input, conv_dim, upsampling_factor):\n","    \"\"\"Dilated Convolutional Block with weight normalization.\n","\n","    Args:\n","        conv_dim: int, determines filter size for the block.\n","        upsampling_factor: int, scale for upsampling.\n","\n","    Returns:\n","        Dilated convolution block.\n","    \"\"\"\n","    conv_t = addon_layers.WeightNormalization(\n","        layers.Conv1DTranspose(conv_dim, 16, upsampling_factor, padding=\"same\"),\n","        data_init=False,\n","    )(input)\n","    lrelu1 = layers.LeakyReLU()(conv_t)\n","    res_stack = residual_stack(lrelu1, conv_dim)\n","    lrelu2 = layers.LeakyReLU()(res_stack)\n","    return lrelu2"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1702195978032,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"oEjtYzYia4sY"},"outputs":[],"source":["def discriminator_block(input):\n","    conv1 = addon_layers.WeightNormalization(\n","        layers.Conv1D(16, 15, 1, \"same\"), data_init=False\n","    )(input)\n","    lrelu1 = layers.LeakyReLU()(conv1)\n","    conv2 = addon_layers.WeightNormalization(\n","        layers.Conv1D(64, 41, 4, \"same\", groups=4), data_init=False\n","    )(lrelu1)\n","    lrelu2 = layers.LeakyReLU()(conv2)\n","    conv3 = addon_layers.WeightNormalization(\n","        layers.Conv1D(256, 41, 4, \"same\", groups=16), data_init=False\n","    )(lrelu2)\n","    lrelu3 = layers.LeakyReLU()(conv3)\n","    conv4 = addon_layers.WeightNormalization(\n","        layers.Conv1D(1024, 41, 4, \"same\", groups=64), data_init=False\n","    )(lrelu3)\n","    lrelu4 = layers.LeakyReLU()(conv4)\n","    conv5 = addon_layers.WeightNormalization(\n","        layers.Conv1D(1024, 41, 4, \"same\", groups=256), data_init=False\n","    )(lrelu4)\n","    lrelu5 = layers.LeakyReLU()(conv5)\n","    conv6 = addon_layers.WeightNormalization(\n","        layers.Conv1D(1024, 5, 1, \"same\"), data_init=False\n","    )(lrelu5)\n","    lrelu6 = layers.LeakyReLU()(conv6)\n","    conv7 = addon_layers.WeightNormalization(\n","        layers.Conv1D(1, 3, 1, \"same\"), data_init=False\n","    )(lrelu6)\n","    return [lrelu1, lrelu2, lrelu3, lrelu4, lrelu5, lrelu6, conv7]"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10868,"status":"ok","timestamp":1702195989931,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"hPhLWoXEa4sZ","outputId":"ca0e4bd1-acc7-4b29-9aad-f35bc1f10f9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, None, 1)]            0         []                            \n","                                                                                                  \n"," mel_spec (MelSpec)          (None, None, 80)             0         ['input_1[0][0]']             \n","                                                                                                  \n"," conv1d (Conv1D)             (None, None, 512)            287232    ['mel_spec[0][0]']            \n","                                                                                                  \n"," leaky_re_lu (LeakyReLU)     (None, None, 512)            0         ['conv1d[0][0]']              \n","                                                                                                  \n"," weight_normalization (Weig  (None, None, 256)            2097921   ['leaky_re_lu[0][0]']         \n"," htNormalization)                                                                                 \n","                                                                                                  \n"," leaky_re_lu_1 (LeakyReLU)   (None, None, 256)            0         ['weight_normalization[0][0]']\n","                                                                                                  \n"," weight_normalization_1 (We  (None, None, 256)            197121    ['leaky_re_lu_1[0][0]']       \n"," ightNormalization)                                                                               \n","                                                                                                  \n"," leaky_re_lu_2 (LeakyReLU)   (None, None, 256)            0         ['weight_normalization_1[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," weight_normalization_2 (We  (None, None, 256)            197121    ['leaky_re_lu_2[0][0]']       \n"," ightNormalization)                                                                               \n","                                                                                                  \n"," add (Add)                   (None, None, 256)            0         ['weight_normalization_2[0][0]\n","                                                                    ',                            \n","                                                                     'leaky_re_lu_1[0][0]']       \n","                                                                                                  \n"," leaky_re_lu_3 (LeakyReLU)   (None, None, 256)            0         ['add[0][0]']                 \n","                                                                                                  \n"," weight_normalization_3 (We  (None, None, 256)            197121    ['leaky_re_lu_3[0][0]']       \n"," ightNormalization)                                                                               \n","                                                                                                  \n"," leaky_re_lu_4 (LeakyReLU)   (None, None, 256)            0         ['weight_normalization_3[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," weight_normalization_4 (We  (None, None, 256)            197121    ['leaky_re_lu_4[0][0]']       \n"," ightNormalization)                                                                               \n","                                                                                                  \n"," add_1 (Add)                 (None, None, 256)            0         ['add[0][0]',                 \n","                                                                     'weight_normalization_4[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," leaky_re_lu_5 (LeakyReLU)   (None, None, 256)            0         ['add_1[0][0]']               \n","                                                                                                  \n"," weight_normalization_5 (We  (None, None, 256)            197121    ['leaky_re_lu_5[0][0]']       \n"," ightNormalization)                                                                               \n","                                                                                                  \n"," leaky_re_lu_6 (LeakyReLU)   (None, None, 256)            0         ['weight_normalization_5[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," weight_normalization_6 (We  (None, None, 256)            197121    ['leaky_re_lu_6[0][0]']       \n"," ightNormalization)                                                                               \n","                                                                                                  \n"," add_2 (Add)                 (None, None, 256)            0         ['weight_normalization_6[0][0]\n","                                                                    ',                            \n","                                                                     'add_1[0][0]']               \n","                                                                                                  \n"," leaky_re_lu_7 (LeakyReLU)   (None, None, 256)            0         ['add_2[0][0]']               \n","                                                                                                  \n"," weight_normalization_7 (We  (None, None, 128)            524673    ['leaky_re_lu_7[0][0]']       \n"," ightNormalization)                                                                               \n","                                                                                                  \n"," leaky_re_lu_8 (LeakyReLU)   (None, None, 128)            0         ['weight_normalization_7[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," weight_normalization_8 (We  (None, None, 128)            49409     ['leaky_re_lu_8[0][0]']       \n"," ightNormalization)                                                                               \n","                                                                                                  \n"," leaky_re_lu_9 (LeakyReLU)   (None, None, 128)            0         ['weight_normalization_8[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," weight_normalization_9 (We  (None, None, 128)            49409     ['leaky_re_lu_9[0][0]']       \n"," ightNormalization)                                                                               \n","                                                                                                  \n"," add_3 (Add)                 (None, None, 128)            0         ['weight_normalization_9[0][0]\n","                                                                    ',                            \n","                                                                     'leaky_re_lu_8[0][0]']       \n","                                                                                                  \n"," leaky_re_lu_10 (LeakyReLU)  (None, None, 128)            0         ['add_3[0][0]']               \n","                                                                                                  \n"," weight_normalization_10 (W  (None, None, 128)            49409     ['leaky_re_lu_10[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_11 (LeakyReLU)  (None, None, 128)            0         ['weight_normalization_10[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_11 (W  (None, None, 128)            49409     ['leaky_re_lu_11[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," add_4 (Add)                 (None, None, 128)            0         ['add_3[0][0]',               \n","                                                                     'weight_normalization_11[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_12 (LeakyReLU)  (None, None, 128)            0         ['add_4[0][0]']               \n","                                                                                                  \n"," weight_normalization_12 (W  (None, None, 128)            49409     ['leaky_re_lu_12[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_13 (LeakyReLU)  (None, None, 128)            0         ['weight_normalization_12[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_13 (W  (None, None, 128)            49409     ['leaky_re_lu_13[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," add_5 (Add)                 (None, None, 128)            0         ['weight_normalization_13[0][0\n","                                                                    ]',                           \n","                                                                     'add_4[0][0]']               \n","                                                                                                  \n"," leaky_re_lu_14 (LeakyReLU)  (None, None, 128)            0         ['add_5[0][0]']               \n","                                                                                                  \n"," weight_normalization_14 (W  (None, None, 64)             131265    ['leaky_re_lu_14[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_15 (LeakyReLU)  (None, None, 64)             0         ['weight_normalization_14[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_15 (W  (None, None, 64)             12417     ['leaky_re_lu_15[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_16 (LeakyReLU)  (None, None, 64)             0         ['weight_normalization_15[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_16 (W  (None, None, 64)             12417     ['leaky_re_lu_16[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," add_6 (Add)                 (None, None, 64)             0         ['weight_normalization_16[0][0\n","                                                                    ]',                           \n","                                                                     'leaky_re_lu_15[0][0]']      \n","                                                                                                  \n"," leaky_re_lu_17 (LeakyReLU)  (None, None, 64)             0         ['add_6[0][0]']               \n","                                                                                                  \n"," weight_normalization_17 (W  (None, None, 64)             12417     ['leaky_re_lu_17[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_18 (LeakyReLU)  (None, None, 64)             0         ['weight_normalization_17[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_18 (W  (None, None, 64)             12417     ['leaky_re_lu_18[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," add_7 (Add)                 (None, None, 64)             0         ['add_6[0][0]',               \n","                                                                     'weight_normalization_18[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_19 (LeakyReLU)  (None, None, 64)             0         ['add_7[0][0]']               \n","                                                                                                  \n"," weight_normalization_19 (W  (None, None, 64)             12417     ['leaky_re_lu_19[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_20 (LeakyReLU)  (None, None, 64)             0         ['weight_normalization_19[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_20 (W  (None, None, 64)             12417     ['leaky_re_lu_20[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," add_8 (Add)                 (None, None, 64)             0         ['weight_normalization_20[0][0\n","                                                                    ]',                           \n","                                                                     'add_7[0][0]']               \n","                                                                                                  \n"," leaky_re_lu_21 (LeakyReLU)  (None, None, 64)             0         ['add_8[0][0]']               \n","                                                                                                  \n"," weight_normalization_21 (W  (None, None, 32)             32865     ['leaky_re_lu_21[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_22 (LeakyReLU)  (None, None, 32)             0         ['weight_normalization_21[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_22 (W  (None, None, 32)             3137      ['leaky_re_lu_22[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_23 (LeakyReLU)  (None, None, 32)             0         ['weight_normalization_22[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_23 (W  (None, None, 32)             3137      ['leaky_re_lu_23[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," add_9 (Add)                 (None, None, 32)             0         ['weight_normalization_23[0][0\n","                                                                    ]',                           \n","                                                                     'leaky_re_lu_22[0][0]']      \n","                                                                                                  \n"," leaky_re_lu_24 (LeakyReLU)  (None, None, 32)             0         ['add_9[0][0]']               \n","                                                                                                  \n"," weight_normalization_24 (W  (None, None, 32)             3137      ['leaky_re_lu_24[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_25 (LeakyReLU)  (None, None, 32)             0         ['weight_normalization_24[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_25 (W  (None, None, 32)             3137      ['leaky_re_lu_25[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," add_10 (Add)                (None, None, 32)             0         ['add_9[0][0]',               \n","                                                                     'weight_normalization_25[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_26 (LeakyReLU)  (None, None, 32)             0         ['add_10[0][0]']              \n","                                                                                                  \n"," weight_normalization_26 (W  (None, None, 32)             3137      ['leaky_re_lu_26[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_27 (LeakyReLU)  (None, None, 32)             0         ['weight_normalization_26[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_27 (W  (None, None, 32)             3137      ['leaky_re_lu_27[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," add_11 (Add)                (None, None, 32)             0         ['weight_normalization_27[0][0\n","                                                                    ]',                           \n","                                                                     'add_10[0][0]']              \n","                                                                                                  \n"," leaky_re_lu_28 (LeakyReLU)  (None, None, 32)             0         ['add_11[0][0]']              \n","                                                                                                  \n"," weight_normalization_28 (W  (None, None, 1)              452       ['leaky_re_lu_28[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n","==================================================================================================\n","Total params: 4646912 (17.73 MB)\n","Trainable params: 4646658 (17.73 MB)\n","Non-trainable params: 254 (929.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["def create_generator(input_shape):\n","    inp = keras.Input(input_shape)\n","    x = MelSpec()(inp)\n","    x = layers.Conv1D(512, 7, padding=\"same\")(x)\n","    x = layers.LeakyReLU()(x)\n","    x = conv_block(x, 256, 8)\n","    x = conv_block(x, 128, 8)\n","    x = conv_block(x, 64, 2)\n","    x = conv_block(x, 32, 2)\n","    x = addon_layers.WeightNormalization(\n","        layers.Conv1D(1, 7, padding=\"same\", activation=\"tanh\")\n","    )(x)\n","    return keras.Model(inp, x)\n","\n","\n","# We use a dynamic input shape for the generator since the model is fully convolutional\n","generator = create_generator((None, 1))\n","generator.summary()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2975,"status":"ok","timestamp":1702195992904,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"2jeLGmzFa4sZ","outputId":"690c5c3c-4dfa-4aed-d9ca-c6a427aed3e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_2 (InputLayer)        [(None, None, 1)]            0         []                            \n","                                                                                                  \n"," average_pooling1d (Average  (None, None, 1)              0         ['input_2[0][0]']             \n"," Pooling1D)                                                                                       \n","                                                                                                  \n"," average_pooling1d_1 (Avera  (None, None, 1)              0         ['average_pooling1d[0][0]']   \n"," gePooling1D)                                                                                     \n","                                                                                                  \n"," weight_normalization_29 (W  (None, None, 16)             273       ['input_2[0][0]']             \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_36 (W  (None, None, 16)             273       ['average_pooling1d[0][0]']   \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_43 (W  (None, None, 16)             273       ['average_pooling1d_1[0][0]'] \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_29 (LeakyReLU)  (None, None, 16)             0         ['weight_normalization_29[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_35 (LeakyReLU)  (None, None, 16)             0         ['weight_normalization_36[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_41 (LeakyReLU)  (None, None, 16)             0         ['weight_normalization_43[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_30 (W  (None, None, 64)             10625     ['leaky_re_lu_29[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_37 (W  (None, None, 64)             10625     ['leaky_re_lu_35[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_44 (W  (None, None, 64)             10625     ['leaky_re_lu_41[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_30 (LeakyReLU)  (None, None, 64)             0         ['weight_normalization_30[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_36 (LeakyReLU)  (None, None, 64)             0         ['weight_normalization_37[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_42 (LeakyReLU)  (None, None, 64)             0         ['weight_normalization_44[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_31 (W  (None, None, 256)            42497     ['leaky_re_lu_30[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_38 (W  (None, None, 256)            42497     ['leaky_re_lu_36[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_45 (W  (None, None, 256)            42497     ['leaky_re_lu_42[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_31 (LeakyReLU)  (None, None, 256)            0         ['weight_normalization_31[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_37 (LeakyReLU)  (None, None, 256)            0         ['weight_normalization_38[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_43 (LeakyReLU)  (None, None, 256)            0         ['weight_normalization_45[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_32 (W  (None, None, 1024)           169985    ['leaky_re_lu_31[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_39 (W  (None, None, 1024)           169985    ['leaky_re_lu_37[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_46 (W  (None, None, 1024)           169985    ['leaky_re_lu_43[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_32 (LeakyReLU)  (None, None, 1024)           0         ['weight_normalization_32[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_38 (LeakyReLU)  (None, None, 1024)           0         ['weight_normalization_39[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_44 (LeakyReLU)  (None, None, 1024)           0         ['weight_normalization_46[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_33 (W  (None, None, 1024)           169985    ['leaky_re_lu_32[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_40 (W  (None, None, 1024)           169985    ['leaky_re_lu_38[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_47 (W  (None, None, 1024)           169985    ['leaky_re_lu_44[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_33 (LeakyReLU)  (None, None, 1024)           0         ['weight_normalization_33[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_39 (LeakyReLU)  (None, None, 1024)           0         ['weight_normalization_40[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_45 (LeakyReLU)  (None, None, 1024)           0         ['weight_normalization_47[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_34 (W  (None, None, 1024)           5244929   ['leaky_re_lu_33[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_41 (W  (None, None, 1024)           5244929   ['leaky_re_lu_39[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_48 (W  (None, None, 1024)           5244929   ['leaky_re_lu_45[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," leaky_re_lu_34 (LeakyReLU)  (None, None, 1024)           0         ['weight_normalization_34[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_40 (LeakyReLU)  (None, None, 1024)           0         ['weight_normalization_41[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," leaky_re_lu_46 (LeakyReLU)  (None, None, 1024)           0         ['weight_normalization_48[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," weight_normalization_35 (W  (None, None, 1)              3075      ['leaky_re_lu_34[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_42 (W  (None, None, 1)              3075      ['leaky_re_lu_40[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n"," weight_normalization_49 (W  (None, None, 1)              3075      ['leaky_re_lu_46[0][0]']      \n"," eightNormalization)                                                                              \n","                                                                                                  \n","==================================================================================================\n","Total params: 16924107 (64.56 MB)\n","Trainable params: 16924086 (64.56 MB)\n","Non-trainable params: 21 (21.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["def create_discriminator(input_shape):\n","    inp = keras.Input(input_shape)\n","    out_map1 = discriminator_block(inp)\n","    pool1 = layers.AveragePooling1D()(inp)\n","    out_map2 = discriminator_block(pool1)\n","    pool2 = layers.AveragePooling1D()(pool1)\n","    out_map3 = discriminator_block(pool2)\n","    return keras.Model(inp, [out_map1, out_map2, out_map3])\n","\n","\n","# We use a dynamic input shape for the discriminator\n","# This is done because the input shape for the generator is unknown\n","discriminator = create_discriminator((None, 1))\n","\n","discriminator.summary()"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1109,"status":"ok","timestamp":1702195994008,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"D6Hg9daja4sZ"},"outputs":[],"source":["# Generator loss\n","\n","\n","def generator_loss(real_pred, fake_pred):\n","    \"\"\"Loss function for the generator.\n","\n","    Args:\n","        real_pred: Tensor, output of the ground truth wave passed through the discriminator.\n","        fake_pred: Tensor, output of the generator prediction passed through the discriminator.\n","\n","    Returns:\n","        Loss for the generator.\n","    \"\"\"\n","    gen_loss = []\n","    for i in range(len(fake_pred)):\n","        gen_loss.append(mse(tf.ones_like(fake_pred[i][-1]), fake_pred[i][-1]))\n","\n","    return tf.reduce_mean(gen_loss)\n","\n","\n","def feature_matching_loss(real_pred, fake_pred):\n","    \"\"\"Implements the feature matching loss.\n","\n","    Args:\n","        real_pred: Tensor, output of the ground truth wave passed through the discriminator.\n","        fake_pred: Tensor, output of the generator prediction passed through the discriminator.\n","\n","    Returns:\n","        Feature Matching Loss.\n","    \"\"\"\n","    fm_loss = []\n","    for i in range(len(fake_pred)):\n","        for j in range(len(fake_pred[i]) - 1):\n","            fm_loss.append(mae(real_pred[i][j], fake_pred[i][j]))\n","\n","    return tf.reduce_mean(fm_loss)\n","\n","\n","def discriminator_loss(real_pred, fake_pred):\n","    \"\"\"Implements the discriminator loss.\n","\n","    Args:\n","        real_pred: Tensor, output of the ground truth wave passed through the discriminator.\n","        fake_pred: Tensor, output of the generator prediction passed through the discriminator.\n","\n","    Returns:\n","        Discriminator Loss.\n","    \"\"\"\n","    real_loss, fake_loss = [], []\n","    for i in range(len(real_pred)):\n","        real_loss.append(mse(tf.ones_like(real_pred[i][-1]), real_pred[i][-1]))\n","        fake_loss.append(mse(tf.zeros_like(fake_pred[i][-1]), fake_pred[i][-1]))\n","\n","    # Calculating the final discriminator loss after scaling\n","    disc_loss = tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss)\n","    return disc_loss"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702195994008,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"BLd9wmfxa4sd"},"outputs":[],"source":["class MelGAN(keras.Model):\n","    def __init__(self, generator, discriminator, **kwargs):\n","        \"\"\"MelGAN trainer class\n","\n","        Args:\n","            generator: keras.Model, Generator model\n","            discriminator: keras.Model, Discriminator model\n","        \"\"\"\n","        super().__init__(**kwargs)\n","        self.generator = generator\n","        self.discriminator = discriminator\n","\n","    def compile(\n","        self,\n","        gen_optimizer,\n","        disc_optimizer,\n","        generator_loss,\n","        feature_matching_loss,\n","        discriminator_loss,\n","    ):\n","        \"\"\"MelGAN compile method.\n","\n","        Args:\n","            gen_optimizer: keras.optimizer, optimizer to be used for training\n","            disc_optimizer: keras.optimizer, optimizer to be used for training\n","            generator_loss: callable, loss function for generator\n","            feature_matching_loss: callable, loss function for feature matching\n","            discriminator_loss: callable, loss function for discriminator\n","        \"\"\"\n","        super().compile()\n","\n","        # Optimizers\n","        self.gen_optimizer = gen_optimizer\n","        self.disc_optimizer = disc_optimizer\n","\n","        # Losses\n","        self.generator_loss = generator_loss\n","        self.feature_matching_loss = feature_matching_loss\n","        self.discriminator_loss = discriminator_loss\n","\n","        # Trackers\n","        self.gen_loss_tracker = keras.metrics.Mean(name=\"gen_loss\")\n","        self.disc_loss_tracker = keras.metrics.Mean(name=\"disc_loss\")\n","\n","    def train_step(self, batch):\n","        x_batch_train, y_batch_train = batch\n","\n","        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","            # Generating the audio wave\n","            gen_audio_wave = generator(x_batch_train, training=True)\n","            # Generating the features using the discriminator\n","            real_pred = discriminator(y_batch_train)\n","            fake_pred = discriminator(gen_audio_wave)\n","            # Calculating the generator losses\n","            gen_loss = generator_loss(real_pred, fake_pred)\n","            fm_loss = feature_matching_loss(real_pred, fake_pred)\n","\n","            # Calculating final generator loss\n","            gen_fm_loss = gen_loss + 10 * fm_loss\n","\n","            # Calculating the discriminator losses\n","            disc_loss = discriminator_loss(real_pred, fake_pred)\n","\n","        # Calculating and applying the gradients for generator and discriminator\n","        grads_gen = gen_tape.gradient(gen_fm_loss, generator.trainable_weights)\n","        grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_weights)\n","        gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_weights))\n","        disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_weights))\n","\n","        self.gen_loss_tracker.update_state(gen_fm_loss)\n","        self.disc_loss_tracker.update_state(disc_loss)\n","\n","        return {\n","            \"gen_loss\": self.gen_loss_tracker.result(),\n","            \"disc_loss\": self.disc_loss_tracker.result(),\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2804831,"status":"error","timestamp":1702198799755,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"wELoHrqxa4se","outputId":"a207a671-3b66-465f-807a-2dd0c3fd0f06"},"outputs":[],"source":["gen_optimizer = keras.optimizers.Adam(\n","    LEARNING_RATE_GEN, beta_1=0.5, beta_2=0.9, clipnorm=1\n",")\n","disc_optimizer = keras.optimizers.Adam(\n","    LEARNING_RATE_DISC, beta_1=0.5, beta_2=0.9, clipnorm=1\n",")\n","\n","# Start training\n","generator = create_generator((None, 1))\n","discriminator = create_discriminator((None, 1))\n","\n","mel_gan = MelGAN(generator, discriminator)\n","mel_gan.compile(\n","    gen_optimizer,\n","    disc_optimizer,\n","    generator_loss,\n","    feature_matching_loss,\n","    discriminator_loss,\n",")\n","mel_gan.fit(\n","    train_dataset.shuffle(200).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), epochs=100\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":743,"status":"ok","timestamp":1702198839439,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"hP3JKFUNa4se","outputId":"044b814d-fd1f-4d9c-ccbe-53d2b2e984a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of audio files: 382\n"]}],"source":["# Splitting the dataset into training and testing splits\n","wavs = tf.io.gfile.glob(\"/content/drive/MyDrive/train_data_real/class_E/*.wav\")\n","print(f\"Number of audio files: {len(wavs)}\")\n","\n","# Mapper function for loading the audio. This function returns two instances of the wave\n","def preprocess(filename):\n","    audio = tf.audio.decode_wav(tf.io.read_file(filename), 1, DESIRED_SAMPLES).audio\n","    return audio\n","\n","\n","# Create tf.data.Dataset objects and apply preprocessing\n","train_dataset = tf.data.Dataset.from_tensor_slices((wavs,))\n","train_dataset = train_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n","train_dataset = train_dataset.map(lambda x: MelSpec()(x))\n","train_dataset = train_dataset.batch(1)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702198844523,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"eYqa_93TCq1E","outputId":"4f1179e0-37b6-4a84-cfaf-ce23bfd71b28"},"outputs":[{"data":{"text/plain":["<_BatchDataset element_spec=TensorSpec(shape=(None, 103, 80), dtype=tf.float32, name=None)>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702198846610,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"M0BiJMessEfz"},"outputs":[],"source":["import os\n","import soundfile as sf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37610,"status":"ok","timestamp":1702198972333,"user":{"displayName":"DEVICHAND Budagam","userId":"03398266847385115021"},"user_tz":-330},"id":"gpBj_6DACs0a","outputId":"791a307e-ee15-4dce-96f0-4e98bb68cacb"},"outputs":[],"source":["x=1\n","sr=52734\n","for audio in train_dataset:\n","  pred = generator.predict(audio, verbose=1)\n","  pred = pred.reshape((26368, 1))\n","  output_file = os.path.join(\"/content/drive/MyDrive/train_data_gen/class_E\", f\"{x}.wav\")\n","  sf.write(output_file, pred, sr)\n","  x=x+1"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
